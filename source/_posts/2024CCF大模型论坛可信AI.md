---
title: 2024CCF大模型论坛可信AI
date: 2024-06-06 23:49:14
tags: 
  - DailyNews
  - 可信AI
  - trustworthy AI
  - CCF
  - thinking
---

在 6 月 6 日的 CCF 大模型论坛上，清华大学长聘教授黄民烈介绍了自己正在做的研究课题，几乎都是围绕大模型安全问题。

黄民烈教授表示，超级对齐Superalignment不仅是安全，本质上应该是怎么样实现自我进化、自我迭代的学习过程，安全问题只是超级对齐的一个外显性质。

（为什么呢？在结尾有解答，他倾向于依靠大模型自我纠错实现安全。我总觉得这个东西有点奇怪，又想不通哪里奇怪……）

黄民烈教授介绍道，目前在超级对齐框架下，其研究团队做了精确对齐算法 EXO。针对大模型攻击，团队做了目标优先级优化（Goal Prioritization），同时研发的模型安全探测器 ShieldLM，可以判断输出内容是否安全。

- 去看EXO
- openai也写了超级对齐的论文

团队还研发了能够弥补人写 Prompt 与模型更能理解的 Prompt 之间差距的黑盒提示优化（Black-box prompt Optimization）。团队还在自动修正模型弱点方面做了大量研究，通过该方法来改进模型的潜力。未来，团队还将在 Reward function 鲁棒性、Human AI 协作和识别新风险等方面继续研究。

（警觉创新点）

OpenAI 在超级对齐的论文里表明是让 GPT-2 监督 GPT-4 训练。

依然有大量的研究问题，但这只是 OpenAI 做了一个非常简单的尝试，我们不应该在这个时点上判断这条技术路径是行不通的。这个探索目前本来就很少，还可以做很多，比如这个模型在不同的任务上训练会怎么样、在不同的模型簇上训练会怎么样等等，还有大量的研究值得我们去做。

（警觉创新点）

训练数据的处理、安全对齐、输出的检测。数据的处理和过滤，确保价值观和意识形态等没有问题。在对齐阶段，要充分考虑安全性和有用性的平衡。输出的检测上要确保安全合规。每个阶段都有重要的算法和工程问题。

OpenAI 的事件并不是说安全不重要，它实际上是商业逻辑和监管逻辑、治理逻辑之间的冲突。它人员的出走是因为组织斗争和政治斗争，跟安全本身的重要性，其实关系没有那么大。他们加入了 Anthropic AI 之后重新领导安全和超级对齐的工作，其实是非常重要的一个事情。

现在的做法本质上是大模型被训练为遵循指令，但是如果我们只要把这种越狱攻击或者其他的攻击包装成一个指令遵循的格式时候，越强的模型它反而越容易遵循违规指令。所以这里面是一个矛和盾的过程，其实它本身既是矛又是盾。“矛”就是我已经被训练成听从指令了，“盾”就是攻击者被包装成一个指令遵循的形式。

论文提出的安全提示优化方法 DRO 是不是已经有相关应用了？

黄民烈：没有，现在还只是科研的阶段。它是一个算法，能够让模型变得更安全一点。

（警觉创新点）

情感AI需要从算法层面避免进入信息茧房。

我们现在还是处在 AGI 的早期。AGI 还需要一个明确的定义，OpenAI 和 Gary Marcus 给了一些定义，但其实是很笼统、很抽象的定义。

整个 AI 发展历史上有很多智能的不同方式，比如符号主义做出来的智能，以感知为主的智能，如人脸识别、视觉识别与认知智能。我们现在处的这一代，就是以数据、知识、算力、算法为核心的认知智能时代。这个时代里，大模型是其中的一个代表，但不是唯一路线，它只是一条目前来讲相对比较成功的路线。

- 去看GPT-4o的技术报告

不太好的地方也很多，推理可信度、可靠性、幻觉等，这些目前还有很多优化空间。我自己其实关心的是怎么能让这个模型能够自我进化、自我提升，然后自动发现模型的漏洞，然后不断提升模型自己的能力和水平。

大模型发展方向预测：首先，我觉得未来肯定会是多模态融合的。然后，具身智能也是很重要的一个方向，通过跟物理世界的交互融合和映射去实现对整个物理世界的理解和建模，是很重要的能力，我觉得这是未来最终的方向。

另外一个方向就是，将工具属性和情感社交属性结合在一起，变成一个真正类人的智能体，这样它既有工具价值，也有社交和情感的价值，两者融合在一起后，就会变成一个真正 AGI 时代的 companion。
